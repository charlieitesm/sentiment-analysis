{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "Natural language processing (NLP) is a branch of artificial intelligence that is focused on enabling computers to understand and process human languages, to get computers closer to a human-level understanding of language. \n",
    "\n",
    "\n",
    "# Tokens\n",
    "\n",
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization:\n",
    "\n",
    "Input: Friends, Romans, Countrymen, lend me your ears; \n",
    "Output: Friends | Romans | Contrymen | lend | me | your | ears\n",
    "\n",
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. Examples:\n",
    "\n",
    "* Walking, walked, walks, walk: Walk\n",
    "* Construction, constructed, constructor: Construct\n",
    "* Catwalk, catty, cats: Cat\n",
    "\n",
    "# Lemmas\n",
    "\n",
    "Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research.\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "2. The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\n",
    "3. The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context; e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation attempts to select the correct lemma depending on the context.\n",
    "\n",
    "# N-Grams\n",
    "\n",
    "In general, n-gram means splitting a string in sequences with the length n. So if we have this string “abcde”, then bigrams are: ab, bc, cd, and de while trigrams will be: abc, bcd, and cde while 4-grams will be abcd, and bcde.\n",
    "\n",
    "Another example would be the phrase: \"I like to eat pancakes in the morning\". If we were to tokenize and group the tokens in groups of 3, that is, 3-Grams or trigrams we would get the following tokens:\n",
    "1. I like to\n",
    "2. like to eat\n",
    "3. to eat pancakes\n",
    "4. eat pancakes in\n",
    "5. pancakes in the\n",
    "6. in the morning\n",
    "\n",
    "# Similarity Functions\n",
    "\n",
    "## Jaccard Index\n",
    "The Jaccard index is a statistic used for gauging the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:\n",
    "\n",
    "<img src=\"images/jaccard_formula.svg\">\n",
    "\n",
    "<img src=\"images/visual_jaccard.png\">\n",
    "\n",
    "Jaccard Index is not limited to NLP applications, we can also use it in Computer Vision:\n",
    "<img src=\"images/stop_sign_jaccard.jpg\">\n",
    "\n",
    "### Practical examples\n",
    "\n",
    "\n",
    "Consider the following phrases:\n",
    "\n",
    "1. The bird is singing\n",
    "2. The dog is barking\n",
    "3. The lady is singing\n",
    "\n",
    "#### Example 1\n",
    "If we were to calculate the Jaccard Index similarity between 1 and 2 we would get:\n",
    "\n",
    "* The union of all tokens: The | bird | dog | is | singing | barking\n",
    "* The intersection of the two sets: The | is\n",
    "\n",
    "We can then calculate the Jaccard Index by calculating the coefficient between the length of the intersection set divided by the length of the union set:\n",
    "\n",
    "Jaccard Index = 2/6 = 0.333 = *33% Similar*\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "If we were to calculate the Jaccard Index similarity between 1 and 3 we would get:\n",
    "\n",
    "* The union of all tokens: The | bird | lady | is | singing \n",
    "* The intersection of the two sets: The | is | singing\n",
    "\n",
    "We can then calculate the Jaccard Index by calculating the coefficient between the length of the intersection set divided by the length of the union set:\n",
    "\n",
    "Jaccard Index = 3/5 =  0.6 = *60% Similar*\n",
    "\n",
    "### Going further\n",
    "\n",
    "Now, consider the following phrases\n",
    "\n",
    "> I won then I lost\n",
    "\n",
    "> I lost then I won\n",
    "\n",
    "If we were to just tokenize them and calculate the Jaccard Index between them, we would get a 100% Similarity (an index of 1.0). However, a person can understand that they do not carry the same meaning given its context. To address this, we could instead use N-Grams\n",
    "\n",
    "#### Using Trigrams\n",
    "* The Union of all tokens: I won then | won then I | then I lost | I lost then | lost then I | then I won\n",
    "* The intersection: NULL\n",
    "\n",
    "Jaccard Index: 0/6 = 0% similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get to coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic spelling checker using Jaccard Index\n",
    "\n",
    "We are writing our own text editor and we would like to implement a basic spelling checker that can catch and suggest corrections for typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Charlie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing: 0.16666666666666666\n",
      "lighting: 0.16666666666666666\n",
      "linking: 0.3333333333333333\n",
      "living: 0.3333333333333333\n",
      "walking: 0.5\n",
      "drawing: 0.6666666666666666\n",
      "orange: 0.7777777777777778\n",
      "bag: 0.8571428571428571\n",
      "apple: 0.875\n",
      "zoo: 1.0\n"
     ]
    }
   ],
   "source": [
    "mistake = \"ligting\"\n",
    " \n",
    "words = ['apple', 'bag', 'drawing', 'listing', 'linking', 'living', 'lighting', 'orange', 'walking', 'zoo']\n",
    "\n",
    "suggestions = []\n",
    "\n",
    "for word in words:\n",
    "    jd = nltk.jaccard_distance(set(mistake), set(word))\n",
    "    suggestions.append((word, jd))\n",
    "\n",
    "suggestions.sort(key=lambda sug: sug[1])\n",
    "\n",
    "for s in suggestions:\n",
    "    print(f\"{s[0]}: {s[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using N-Grams with Jaccard\n",
    "\n",
    "Let's check how similar two text are, we'll first calculate according to its unigrams and we'll see an example of how we can carry contextual meaning into our distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I won then I lost\",\n",
    "    \"I lost then I won\",\n",
    "    \"It might help to re-install Python if possible.\",\n",
    "    \"It can help to install Python again if possible.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [['I', 'won', 'then', 'I', 'lost'], ['I', 'lost', 'then', 'I', 'won'], ['It', 'might', 'help', 'to', 're-install', 'Python', 'if', 'possible', '.'], ['It', 'can', 'help', 'to', 'install', 'Python', 'again', 'if', 'possible', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [nltk.word_tokenize(s) for s in sentences]\n",
    "\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [{('I',), ('won',), ('then',), ('lost',)}, {('I',), ('won',), ('then',), ('lost',)}, {('help',), ('Python',), ('.',), ('if',), ('to',), ('It',), ('might',), ('re-install',), ('possible',)}, {('help',), ('Python',), ('.',), ('if',), ('to',), ('can',), ('It',), ('install',), ('possible',), ('again',)}]\n",
      "\n",
      "*********************\n",
      "\n",
      "Trigrams: [{('I', 'won', 'then'), ('won', 'then', 'I'), ('then', 'I', 'lost')}, {('I', 'lost', 'then'), ('then', 'I', 'won'), ('lost', 'then', 'I')}, {('help', 'to', 're-install'), ('It', 'might', 'help'), ('Python', 'if', 'possible'), ('might', 'help', 'to'), ('re-install', 'Python', 'if'), ('to', 're-install', 'Python'), ('if', 'possible', '.')}, {('Python', 'again', 'if'), ('install', 'Python', 'again'), ('It', 'can', 'help'), ('to', 'install', 'Python'), ('help', 'to', 'install'), ('can', 'help', 'to'), ('again', 'if', 'possible'), ('if', 'possible', '.')}]\n"
     ]
    }
   ],
   "source": [
    "unigrams = [set(nltk.ngrams(tk, n=1)) for tk in tokens]\n",
    "\n",
    "trigrams = [set(nltk.ngrams(tk, n=3)) for tk in tokens]\n",
    "\n",
    "print(f\"Unigrams: {unigrams}\")\n",
    "\n",
    "print(\"\\n*********************\\n\")\n",
    "\n",
    "print(f\"Trigrams: {trigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the following phrases:\n",
      "I won then I lost\n",
      "I lost then I won\n",
      "\n",
      "*********\n",
      "\n",
      "Jaccard distance between sentence 1 and 2 using unigrams: 0.0\n",
      "Jaccard distance between sentence 1 and 2 using trigrams: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Jaccard distance between sentence 1 and 2\n",
    "\n",
    "print(\"Comparing the following phrases:\")\n",
    "print(sentences[0])\n",
    "print(sentences[1])\n",
    "\n",
    "print(\"\\n*********\\n\")\n",
    "\n",
    "print(f\"Jaccard distance between sentence 1 and 2 using unigrams: {nltk.jaccard_distance(unigrams[0], unigrams[1])}\")\n",
    "print(f\"Jaccard distance between sentence 1 and 2 using trigrams: {nltk.jaccard_distance(trigrams[0], trigrams[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the following phrases:\n",
      "It might help to re-install Python if possible.\n",
      "It can help to install Python again if possible.\n",
      "\n",
      "*********\n",
      "\n",
      "Jaccard distance between sentence 1 and 2 using unigrams: 0.4166666666666667\n",
      "Jaccard distance between sentence 1 and 2 using trigrams: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "# Jaccard distance between sentece 3 and\n",
    "\n",
    "print(\"Comparing the following phrases:\")\n",
    "print(sentences[2])\n",
    "print(sentences[3])\n",
    "\n",
    "print(\"\\n*********\\n\")\n",
    "\n",
    "print(f\"Jaccard distance between sentence 3 and 4 using unigrams: {nltk.jaccard_distance(unigrams[2], unigrams[3])}\")\n",
    "print(f\"Jaccard distance between sentence 3 and 4 using trigrams: {nltk.jaccard_distance(trigrams[2], trigrams[3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
